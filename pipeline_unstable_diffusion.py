# @title UnstableDiffusionPipeline
# See the following web page for the usage.
# https://github.com/nanashi161382/unstable_diffusion/tree/main
from diffusers import (
    StableDiffusionInpaintPipelineLegacy,
    DiffusionPipeline,
    DPMSolverMultistepScheduler,
)
from IPython.display import display
import numpy as np
import PIL
import re
import torch
from torch import autocast
from typing import Optional, List, Union, Callable, Tuple


class PipelineType:
    def __init__(self, rand_seed: Optional[int]):
        """
        Args:
            rand_seed (`int`, *optional*):
                A random seed for [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation deterministic.
        """
        self._rand_seed = rand_seed
        self._generator = None

    def InitializeGenerator(self, pipe):
        if not self._rand_seed:
            return None
        if not self._generator:
            self._generator = torch.Generator(device=pipe.device.type)
        self._generator.manual_seed(self._rand_seed)
        print(f"Setting random seed to {self._rand_seed}")

    def GetGenerator(self):
        return self._generator

    def Rand(self, shape, device, dtype):
        generator = self.GetGenerator()
        if device.type == "mps":
            # randn does not work reproducibly on mps
            return torch.randn(
                shape,
                generator=generator,
                device="cpu",
                dtype=dtype,
            ).to(device)
        else:
            return torch.randn(
                shape,
                generator=generator,
                device=device,
                dtype=dtype,
            )

    # Modified a copy from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline.get_timesteps
    def GetTimestepsWithStrength(self, pipe, num_inference_steps, strength):
        # get the original timestep using init_timestep
        offset = pipe.scheduler.config.get("steps_offset", 0)
        init_timestep = int(num_inference_steps * strength) + offset
        init_timestep = min(init_timestep, num_inference_steps)

        # Some schedulers like PNDM have timesteps as arrays
        # It's more optimized to move all timesteps to correct device beforehand
        t_start = max(num_inference_steps - init_timestep + offset, 0)
        timesteps = pipe.scheduler.timesteps[t_start:].to(pipe.device)

        latent_timestep = timesteps[:1]

        return timesteps, latent_timestep

    @classmethod
    def GetLatentsShape(
        cls, size: Tuple[int, int], batch_size, num_channels, scale_factor
    ):
        """
        Args:
            size (`(int, int)`, *optional*, defaults to (512, 512))
                The (width, height) pair in pixels of the generated image.
            batch_size:
                Always 1
            num_channels:
                unet.in_channels
            scale_factor:
                VAE scale factor
        """
        width, height = size
        if height % scale_factor != 0 or width % scale_factor != 0:
            print(
                f"`width` and `height` have to be divisible by {scale_factor}. "
                "Automatically rounded."
            )

        # get the initial random noise unless the user supplied it
        latents_shape = (
            batch_size,
            num_channels,
            height // scale_factor,
            width // scale_factor,
        )
        return latents_shape


class ByLatents:
    def __init__(self, latents: torch.FloatTensor):
        """
        Args:
            latents (`torch.FloatTensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image generation. Can be used to tweak the same generation with different prompts. If not provided, a latents tensor will ge generated by sampling using the supplied random `generator`.
        """
        self._latents = latents

    def GetLatents(self, txt2img, latents_shape, device, dtype):
        if latents.shape != latents_shape:
            raise ValueError(
                f"Unexpected latents shape, got {latents.shape}, expected {latents_shape}"
            )
        return latents.to(device=device, dtype=dtype)


class Randomly:
    def __init__(self, symmetric: Optional[bool] = False):
        self._symmetric = symmetric

    def GetLatents(self, txt2img, latents_shape, device, dtype):
        if not self._symmetric:
            latents = txt2img.Rand(latents_shape, device, dtype)
            return latents

        # Making symmetric latents.
        # `(latents + latents.flip(-1)) / 2.0` didn't work.
        width = latents_shape[-1]
        half_width = int((width + 1) / 2)
        half_shape = latents_shape[:-1] + (half_width,)
        left = txt2img.Rand(half_shape, device, dtype)
        right = left.flip(-1)
        extra_width = (half_width * 2) - width
        if extra_width > 0:
            right = right[:, :, :, extra_width:]
        return torch.cat([left, right], dim=-1)


class Txt2Img(PipelineType):
    def __init__(
        self,
        initialize: Union[ByLatents, Randomly],
        size: Tuple[int, int] = (512, 512),
        rand_seed: Optional[int] = None,
    ):
        """
        Args:
            size (`(int, int)`, *optional*, defaults to (512, 512))
                The (width, height) pair in pixels of the generated image.
            initialize (`ByLatents` or `Randomly`):
            rand_seed (`int`, *optional*):
                A random seed for [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation deterministic.
        """
        super().__init__(rand_seed)
        self._size = size
        self._initialize = initialize

    def GetInitialLatentsAndTimesteps(self, pipe, dtype, num_inference_steps: int):
        scale_factor = pipe.image_model.vae_scale_factor()

        latents_shape = PipelineType.GetLatentsShape(
            self._size, 1, pipe.unet.in_channels, scale_factor
        )
        latents = self._initialize.GetLatents(self, latents_shape, pipe.device, dtype)

        # scale the initial noise by the standard deviation required by the scheduler
        latents = latents * pipe.scheduler.init_noise_sigma

        # Some schedulers like PNDM have timesteps as arrays
        # It's more optimized to move all timesteps to correct device beforehand
        timesteps = pipe.scheduler.timesteps.to(pipe.device)

        return latents, timesteps, None


class Img2Img(PipelineType):
    def __init__(
        self,
        init_image: PIL.Image.Image,
        strength: float = 0.8,
        rand_seed: Optional[int] = None,
    ):
        """
        Args:
            init_image `PIL.Image.Image`:
                `Image`, or tensor representing an image batch, that will be used as the starting point for the process.
            strength (`float`, *optional*, defaults to 0.8):
                Conceptually, indicates how much to transform the reference `init_image`. Must be between 0 and 1.
                `init_image` will be used as a starting point, adding more noise to it the larger the `strength`. The number of denoising steps depends on the amount of noise initially added. When `strength` is 1, added noise will be maximum and the denoising process will run for the full number of iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores `init_image`.
            rand_seed (`int`, *optional*):
                A random seed for [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation deterministic.
        """
        super().__init__(rand_seed)
        if strength < 0 or strength > 1:
            raise ValueError(
                f"The value of strength should in [0.0, 1.0] but is {strength}"
            )
        self._strength = strength
        self._init_image = init_image

    def GetInitialLatentsAndTimesteps(self, pipe, dtype, num_inference_steps: int):
        generator = self.GetGenerator()

        init_latents = pipe.image_model.Encode(self._init_image, dtype, generator)
        noise = self.Rand(init_latents.shape, pipe.device, dtype)
        timesteps, latent_timestep = self.GetTimestepsWithStrength(
            pipe, num_inference_steps, self._strength
        )

        init_latents_noise = pipe.scheduler.add_noise(
            init_latents, noise, latent_timestep
        )

        return init_latents_noise, timesteps, None


class Inpaint(PipelineType):
    def __init__(
        self,
        init_image: PIL.Image.Image,
        mask_image: PIL.Image.Image,
        strength: float = 0.8,
        rand_seed: Optional[int] = None,
    ):
        """
        Args:
            init_image `PIL.Image.Image`:
                `Image`, or tensor representing an image batch, that will be used as the starting point for the process. This is the image whose masked region will be inpainted.
            mask_image `PIL.Image.Image`:
                `Image`, or tensor representing an image batch, to mask `init_image`. White pixels in the mask will be replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.
            strength (`float`, *optional*, defaults to 0.8):
                Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When `strength` is 1, the denoising process will be run on the masked area for the full number of iterations specified in `num_inference_steps`. `init_image` will be used as a reference for the masked area, adding more noise to that region the larger the `strength`. If `strength` is 0, no inpainting will occur.
            rand_seed (`int`, *optional*):
                A random seed for [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation deterministic.
        """
        super().__init__(rand_seed)
        if strength < 0 or strength > 1:
            raise ValueError(
                f"The value of strength should in [0.0, 1.0] but is {strength}"
            )
        self._strength = strength
        self._init_image = init_image
        self._mask_image = mask_image

    def GetInitialLatentsAndTimesteps(self, pipe, dtype, num_inference_steps: int):
        generator = self.GetGenerator()

        init_latents = pipe.image_model.Encode(self._init_image, dtype, generator)
        mask = pipe.image_model.PreprocessMask(self._mask_image, dtype)
        noise = self.Rand(init_latents.shape, pipe.device, dtype)
        apply_mask = self.ApplyMask(pipe, init_latents, mask, noise)

        # check sizes
        if not mask.shape == init_latents.shape:
            raise ValueError("The mask and init_image should be the same size!")

        timesteps, latent_timestep = self.GetTimestepsWithStrength(
            pipe, num_inference_steps, self._strength
        )

        # add noise to latents using the timesteps
        init_latents_noise = apply_mask.AddNoise(latent_timestep)

        return init_latents_noise, timesteps, apply_mask

    class ApplyMask:
        def __init__(self, pipe, init_latents, mask, noise):
            self._pipe = pipe
            self._init_latents = init_latents
            self._mask = mask
            self._noise = noise

        def AddNoise(self, ts):
            return self._pipe.scheduler.add_noise(self._init_latents, self._noise, ts)

        def __call__(self, latents, t):
            init_latents_noise = self.AddNoise(torch.tensor([t]))
            return (init_latents_noise * self._mask) + (latents * (1 - self._mask))


class ImageModel:
    def __init__(self, vae, vae_scale_factor, device):
        """
        Args:
            vae:
            vae_scale_factor:
            device:
        """
        self._vae = vae
        self._vae_scale_factor = vae_scale_factor
        self._device = device

    def vae_scale_factor(self):
        return self._vae_scale_factor

    def Preprocess(self, image: PIL.Image.Image):
        w, h = image.size
        # Shouldn't this be consistent with vae_scale_factor?
        w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32
        image = image.resize((w, h), resample=PIL.Image.LANCZOS)
        image = np.array(image).astype(np.float32) / 255.0
        image = image[None].transpose(0, 3, 1, 2)
        image = torch.from_numpy(image)
        return 2.0 * image - 1.0

    def Encode(
        self, image: PIL.Image.Image, dtype, generator: Optional[torch.Generator]
    ):
        image = self.Preprocess(image).to(device=self._device, dtype=dtype)

        # encode the init image into latents and scale the latents
        latent_dist = self._vae.encode(image).latent_dist
        latents = latent_dist.sample(generator=generator)
        latents = 0.18215 * latents

        return latents

    def Decode(self, latents):
        latents = 1 / 0.18215 * latents
        image = self._vae.decode(latents).sample
        image = (image / 2 + 0.5).clamp(0, 1)
        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16
        image = image.cpu().permute(0, 2, 3, 1).float().numpy()
        return DiffusionPipeline.numpy_to_pil(image)

    def PreprocessMask(self, mask: PIL.Image.Image, dtype):
        scale_factor = self.vae_scale_factor()
        # preprocess mask
        mask = mask.convert("L")
        w, h = mask.size
        # Shouldn't this be consistent with vae_scale_factor?
        w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32
        mask = mask.resize(
            (w // scale_factor, h // scale_factor), resample=PIL.Image.NEAREST
        )
        mask = np.array(mask).astype(np.float32) / 255.0
        mask = np.tile(mask, (4, 1, 1))
        mask = mask[None].transpose(0, 1, 2, 3)  # what does this step do?
        mask = 1 - mask  # repaint white, keep black
        mask = torch.from_numpy(mask).to(device=self._device, dtype=dtype)
        return mask


class TextEmbeddings:
    def __init__(self, text_embedding):
        self._text_embeddings = [text_embedding]
        self._expiry = [-1]
        self.dtype = text_embedding.dtype

    def AddNext(self, start, next_embedding):
        self._expiry[-1] = start
        self._expiry.append(-1)
        # TODO: support more than 1 internal embeddings.
        self._text_embeddings.append(next_embedding._text_embeddings[0])

    def Get(self, i):
        for k, e in enumerate(self._expiry):
            if (e == -1) or (i < e):
                return self._text_embeddings[k]
        print(f"text embeddings: index out of bounds: {i}")
        print("returning the last text embeddings.")
        return self._text_embeddings[-1]


class TextEmbeddingsCat:
    def __init__(self, text_embeddings_ls):
        self._text_embeddings_ls = text_embeddings_ls
        self.dtype = text_embeddings_ls[0].dtype

    def Get(self, i):
        return torch.cat([t.Get(i) for t in self._text_embeddings_ls])


class BaseEncoding:
    def EncodePrompt(self, text_model):
        raise NotImplementedError()

    def EncodeNegativePrompt(self, text_model):
        raise NotImplementedError()


class StandardEncoding(BaseEncoding):
    def __init__(
        self,
        prompt: str,
        negative_prompt: Optional[str] = None,
    ):
        """
        Args:
            prompt (`str`):
                The prompt or prompts to guide the image generation.
            negative_prompt (`str`, *optional*):
                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).
        """
        self._prompt = prompt
        if negative_prompt is None:
            negative_prompt = ""
        self._negative_prompt = negative_prompt

    def EncodeText(self, text_model, text: str):
        return TextEmbeddings(text_model.EncodeText(text)[0])

    def EncodePrompt(self, text_model):
        return self.EncodeText(text_model, self._prompt)

    def EncodeNegativePrompt(self, text_model):
        return self.EncodeText(text_model, self._negative_prompt)

    # *** Utility functions for subclasses below ***
    @classmethod
    def AddOrInit(cls, v, x):
        if v is None:
            return x
        else:
            return v + x

    @classmethod
    def GetAnnotation(cls, chunk: str):
        words = []
        weight = 1.0
        repeat = 1
        for word in [x.strip() for x in chunk.split(" ")]:
            if not word:
                continue
            if word[0] == ":":
                if len(word) == 1:
                    raise ValueError(f"Don't put any spaces after :")
                weight = float(word[1:])
            elif word[0] == "*":
                if len(word) == 1:
                    raise ValueError(f"Don't put any spaces after *")
                repeat = int(word[1:])
            else:
                words.append(word)
        if weight < 0.0:
            raise ValueError(f"Invalid weight: {weight}")
        if repeat < 1:
            raise ValueError(f"Invalid repeat: {repeat}")
        return " ".join(words), weight, repeat

    class AdjustingBase:
        def __init__(self):
            self.unweighted_mean = None

        def Add(self, enc, unweighted_mean):
            self.unweighted_mean = enc.AddOrInit(self.unweighted_mean, unweighted_mean)

        def Average(self, chunk_len):
            chunk_len = torch.tensor(chunk_len).to(self.unweighted_mean.dtype)
            self.unweighted_mean /= chunk_len

        def AdjustMean(self, states, device):
            new_mean = states.float().mean(axis=[-2, -1]).to(states.dtype)
            states *= (self.unweighted_mean / new_mean).unsqueeze(-1)
            return torch.cat([torch.unsqueeze(states, 0)], dim=0).to(device)


class ShiftEncoding(StandardEncoding):
    def __init__(
        self,
        prompt: str,
        negative_prompt: Optional[str] = None,
        reverse: bool = True,
        rotate: bool = True,
        convolve: Union[bool, List[float]] = False,
    ):
        super().__init__(prompt, negative_prompt)
        self._reverse = reverse
        self._rotate = rotate
        if not isinstance(convolve, list):
            if convolve:
                convolve = [0.7, 0.23, 0.07]
            else:
                convolve = [1.0]
        self._convolve = np.array(convolve)

    class Parallel(StandardEncoding.AdjustingBase):
        def __init__(self):
            super().__init__()
            self.states = None

        def Add(self, enc, states, unweighted_mean):
            super().Add(enc, unweighted_mean)
            self.states = enc.AddOrInit(self.states, states)

        def Average(self, chunk_len):
            super().Average(chunk_len)
            self.states /= chunk_len

        def ToStates(self, *input, **kwargs):
            return self.states

    class ChunkedPrompt:
        class PhraseInfo:
            def __init__(self, phrases, weights, token_nums):
                self.phrases = phrases
                self.weights = weights
                self.token_nums = token_nums

            def Len(self):
                return len(self.phrases)

            def Reverse(self):
                self.phrases.reverse()
                self.weights.reverse()
                self.token_nums.reverse()

        def __init__(self, text_model, text: str):
            parts = [x.strip() for x in re.split(">>>+", text)]
            if len(parts) > 2:
                raise ValueError(
                    f'">>>" should appear at most once in a prompt, '
                    f"but appeared {len(parts) - 1} times: {text}"
                )
            elif len(parts) == 2:
                anc, unanc = parts
            else:
                anc = ""
                unanc = text
            self.anchored = self.Parse(text_model, anc)
            self.unanchored = self.Parse(text_model, unanc)

        def Parse(self, text_model, text):
            if not text:
                return self.PhraseInfo([], [], [])

            chunks = [x.strip() for x in text.split(",")]
            annotations = [
                StandardEncoding.GetAnnotation(chunk) for chunk in chunks if chunk
            ]
            phrases = []
            weights = []
            token_nums = []
            for phrase, weight, repeat in annotations:
                num_tokens = text_model.num_tokens(phrase)
                for i in range(repeat):
                    phrases.append(phrase)
                    weights.append(weight)
                    token_nums.append(num_tokens)
            return self.PhraseInfo(phrases, weights, token_nums)

        def ShiftRange(self):
            i = self.unanchored.Len()
            return i if i > 0 else 1

        def Shift(self, i: int, rotate: bool):
            anc = self.anchored
            unanc = self.unanchored
            if rotate:
                return self.PhraseInfo(
                    anc.phrases + unanc.phrases[i:] + unanc.phrases[:i],
                    anc.weights + unanc.weights[i:] + unanc.weights[:i],
                    anc.token_nums + unanc.token_nums[i:] + unanc.token_nums[:i],
                )
            else:
                return self.PhraseInfo(
                    anc.phrases + unanc.phrases[i:],
                    anc.weights + unanc.weights[i:],
                    anc.token_nums + unanc.token_nums[i:],
                )

        def Reverse(self):
            self.unanchored.Reverse()

    def GetWeights(self, text_model, shifted):
        weights = [1.0]
        for w, n in zip(shifted.weights, shifted.token_nums):
            weights += [w] * n
        weights = (
            np.convolve(np.array(weights) - 1.0, self._convolve, mode="full") + 1.0
        )
        if text_model.debug:  # Debug output
            display(weights)
        if len(weights) >= text_model.max_length():
            weights = weights[: text_model.max_length()]
        weights = np.concatenate(
            [
                weights,
                np.array([1.0] * (text_model.max_length() - len(weights))),
            ]
        )
        return weights

    def EncodeText(self, text_model, text: str):
        chunked = self.ChunkedPrompt(text_model, text)
        proc = self.Parallel()

        def run():
            shift_range = chunked.ShiftRange()
            for i in range(shift_range):
                shifted = chunked.Shift(i, self._rotate)
                shifted_text = " ".join(shifted.phrases)
                weights = self.GetWeights(text_model, shifted)
                if text_model.debug:  # Debug output
                    print(shifted_text)

                embeddings = text_model.EncodeText(shifted_text, False)
                unweighted_mean = (
                    embeddings[0][0].float().mean(axis=[-2, -1]).to(embeddings[0].dtype)
                )
                weights = (
                    torch.tensor(weights)
                    .to(device=text_model._device, dtype=embeddings[0].dtype)
                    .unsqueeze(-1)
                )
                full = embeddings[0][0] * weights
                proc.Add(self, full, unweighted_mean)
            return shift_range

        n = run()
        if self._reverse:
            chunked.Reverse()
            n += run()
        proc.Average(n)

        new_states = proc.ToStates(text_model.max_length(), 1)
        return TextEmbeddings(proc.AdjustMean(new_states, text_model._device))


class ListEncoding(BaseEncoding):
    def __init__(self, encodings: List[Tuple[int, StandardEncoding]]):
        if not encodings:
            raise ValueError("ListEncoding requires at least 1 sub-encoding.")
        if encodings[0][0] != 0:
            raise ValueError("Initial encoding must start from 0.")
        self._encodings = encodings

    def EncodePrompt(self, text_model):
        embeddings = None
        for start, encoding in self._encodings:
            emb = encoding.EncodePrompt(text_model)
            if not embeddings:
                embeddings = emb
            else:
                embeddings.AddNext(start, emb)
        return embeddings

    def EncodeNegativePrompt(self, text_model):
        embeddings = None
        for start, encoding in self._encodings:
            emb = encoding.EncodeNegativePrompt(text_model)
            if not embeddings:
                embeddings = emb
            else:
                embeddings.AddNext(start, emb)
        return embeddings


class TextModel:
    def __init__(self, tokenizer, text_encoder, device, debug):
        """
        Args:
            tokenizer:
            text_encoder:
            device:
            debug:
        """
        self._tokenizer = tokenizer
        self._text_encoder = text_encoder
        self._device = device
        self.SetDebug(debug)

    def SetDebug(self, on: bool):
        self.debug = on

    def max_length(self):
        return self._tokenizer.model_max_length

    def num_tokens(self, text: str):
        tokens = self._tokenizer(text)
        return len(tokens.input_ids) - 2

    def tokenize(self, text: str):
        max_length = self.max_length()
        return self._tokenizer(
            text,
            padding="max_length",
            max_length=max_length,
            return_tensors="pt",
        )

    def decode_tokens(self, tokens):
        return self._tokenizer.batch_decode(tokens)

    def EncodeText(self, text: str, fail_on_truncation: bool = True):
        max_length = self.max_length()
        text_inputs = self.tokenize(text)
        text_input_ids = text_inputs.input_ids
        attention_mask = text_inputs.attention_mask

        if text_input_ids.shape[-1] > max_length:
            removed_text = self.decode_tokens(text_input_ids[:, max_length:])
            if fail_on_truncation:
                raise ValueError(
                    "The following part of your input was truncated because CLIP can only handle sequences up to"
                    f" {max_length} tokens: {removed_text}"
                )
            text_input_ids = text_input_ids[:, :max_length]
        embeddings = self._text_encoder(
            text_input_ids.to(self._device),
            attention_mask=None,
        )

        num_tokens = 0
        for m in attention_mask[0]:
            if m.item() > 0:
                num_tokens += 1
        return embeddings[0], embeddings[1], num_tokens

    def GetTextEmbeddings(
        self,
        text_input: StandardEncoding,
        do_classifier_free_guidance: bool,
    ):
        # get prompt text embeddings
        text_embeddings = text_input.EncodePrompt(self)

        # get unconditional embeddings for classifier free guidance
        if do_classifier_free_guidance:
            uncond_embeddings = text_input.EncodeNegativePrompt(self)

            # For classifier free guidance, we need to do two forward passes.
            # Here we concatenate the unconditional and text embeddings into a single batch
            # to avoid doing two forward passes
            # text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
            text_embeddings = TextEmbeddingsCat([uncond_embeddings, text_embeddings])

        return text_embeddings


class UnstableDiffusionPipeline:
    def __init__(self, devicetype: str = "cuda"):
        self._devicetype_str = devicetype
        self.SetDebug()

    def SetDebug(self, on: bool = False):
        self.debug = on

    def GetRevision(self, dataset: str, revision: Optional[str] = None):
        # revision is a git branch name assigned to the model repository.

        # Always return the provided revision if any.
        if revision:
            return revision

        # We always have the "main" branch.
        default_revision = "main"
        # There may be other branches for smaller memory footprint
        recommended_revision = {
            "stabilityai/stable-diffusion-2": "fp16",
            "CompVis/stable-diffusion-v1-4": "fp16",
            "runwayml/stable-diffusion-v1-5": "fp16",
            "hakurei/waifu-diffusion": "fp16",
            "naclbit/trinart_stable_diffusion_v2": "diffusers-60k",
        }

        return recommended_revision.get(dataset, default_revision)

    def Connect(
        self,
        dataset: str,
        revision: Optional[str] = None,
        auth_token: Optional[str] = None,
        use_xformers: bool = False,
    ):
        extra_args = {
            "torch_dtype": torch.float32,
            "revision": self.GetRevision(dataset, revision),
        }
        if auth_token:
            extra_args["use_auth_token"] = auth_token

        # use DPM-Solver++ scheduler
        scheduler = DPMSolverMultistepScheduler.from_pretrained(
            dataset, subfolder="scheduler"
        )
        extra_args["scheduler"] = scheduler

        # Prepare the StableDiffusion pipeline.
        pipe = StableDiffusionInpaintPipelineLegacy.from_pretrained(
            dataset, **extra_args
        ).to(self._devicetype_str)

        # Options for efficient execution
        pipe.enable_attention_slicing()
        if use_xformers and (self._devicetype_str == "cuda"):
            pipe.enable_xformers_memory_efficient_attention()

        return self.SetPipeline(pipe)

    def SetPipeline(self, pipe):
        self._pipe = pipe
        self.device = self._pipe.device
        self.unet = self._pipe.unet
        self.scheduler = self._pipe.scheduler
        self.text_model = TextModel(
            pipe.tokenizer, pipe.text_encoder, pipe.device, self.debug
        )
        self.image_model = ImageModel(pipe.vae, pipe.vae_scale_factor, pipe.device)
        return self

    def progress_bar(self, *input, **kwargs):
        return self._pipe.progress_bar(*input, **kwargs)

    def ShowDetailedSteps(self, interval: int = 0):
        """
        Args:
            interval (`int`, default to 0):
                if interval = 0 or less, it won't show detailed steps.
        """
        self._detailed_interval = interval

    def InspectLatents(self, model_output, timestep, sample):
        """
        This is to inspect the noise (= model output) during image generation.
        Calling the scheduler to recover latents from noise changing the scheduler's internal state
        and thus affects the image generation.
        So this reimplement the function so as not to change the internal state by copying and
        modifying DPMSolverMultistepScheduler.step() available at
        https://github.com/huggingface/diffusers/blob/769f0be8fb41daca9f3cbcffcfd0dbf01cc194b8/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L428
        The implementation depends on scheduler, so this function only supports
        DPMSolverMultistepScheduler.
        """
        if not isinstance(self.scheduler, DPMSolverMultistepScheduler):
            raise ValueError("Unsupported scheduler for inspection.")
        timestep = timestep.to(self.scheduler.timesteps.device)
        step_index = (self.scheduler.timesteps == timestep).nonzero()
        if len(step_index) == 0:
            step_index = len(self.scheduler.timesteps) - 1
        else:
            step_index = step_index.item()
        prev_timestep = (
            0
            if step_index == len(self.scheduler.timesteps) - 1
            else self.scheduler.timesteps[step_index + 1]
        )
        lower_order_final = (
            (step_index == len(self.scheduler.timesteps) - 1)
            and self.scheduler.config.lower_order_final
            and len(self.scheduler.timesteps) < 15
        )
        lower_order_second = (
            (step_index == len(self.scheduler.timesteps) - 2)
            and self.scheduler.config.lower_order_final
            and len(self.scheduler.timesteps) < 15
        )
        model_output = self.scheduler.convert_model_output(
            model_output, timestep, sample
        )
        if (
            self.scheduler.config.solver_order == 1
            or self.scheduler.lower_order_nums < 1
            or lower_order_final
        ):
            return self.scheduler.dpm_solver_first_order_update(
                model_output, timestep, prev_timestep, sample
            )
        elif (
            self.scheduler.config.solver_order == 2
            or self.scheduler.lower_order_nums < 2
            or lower_order_second
        ):
            timestep_list = [self.scheduler.timesteps[step_index - 1], timestep]
            return self.scheduler.multistep_dpm_solver_second_order_update(
                self.scheduler.model_outputs, timestep_list, prev_timestep, sample
            )
        else:
            timestep_list = [
                self.scheduler.timesteps[step_index - 2],
                self.scheduler.timesteps[step_index - 1],
                timestep,
            ]
            return self.scheduler.multistep_dpm_solver_third_order_update(
                self.scheduler.model_outputs, timestep_list, prev_timestep, sample
            )

    def Generate(
        self,
        text_embeddings,
        do_classifier_free_guidance: bool,
        pipeline_type: Union[Txt2Img, Img2Img, Inpaint],
        guidance_scale: float = 7.5,
        num_inference_steps: int = 50,
        eta: float = 0.0,
    ):
        # set timesteps and initial latents
        pipeline_type.InitializeGenerator(self)
        self.scheduler.set_timesteps(num_inference_steps)
        (latents, timesteps, apply_mask,) = pipeline_type.GetInitialLatentsAndTimesteps(
            self, text_embeddings.dtype, num_inference_steps
        )

        # 8. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
        extra_step_kwargs = self._pipe.prepare_extra_step_kwargs(
            pipeline_type.GetGenerator(), eta
        )

        # showing initial latent if details are required
        latents_ls = [latents] if self._detailed_interval else []
        for i, t in enumerate(self.progress_bar(timesteps)):
            # expand the latents if we are doing classifier free guidance
            latent_model_input = (
                torch.cat([latents] * 2) if do_classifier_free_guidance else latents
            )
            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)

            # predict the noise residual
            noise_pred = self.unet(
                latent_model_input, t, encoder_hidden_states=text_embeddings.Get(i)
            ).sample

            # perform guidance
            if do_classifier_free_guidance:
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + guidance_scale * (
                    noise_pred_text - noise_pred_uncond
                )

            # showing detailed steps
            show_detailed_steps = self._detailed_interval and (
                (i + 1) % self._detailed_interval == 0
            )
            if show_detailed_steps:
                if do_classifier_free_guidance:
                    latents_ls.append(self.InspectLatents(noise_pred_text, t, latents))
                    latents_ls.append(
                        self.InspectLatents(noise_pred_uncond, t, latents)
                    )

            # compute the previous noisy sample x_t -> x_t-1
            latents = self.scheduler.step(
                noise_pred, t, latents, **extra_step_kwargs
            ).prev_sample

            # showing detailed steps
            if show_detailed_steps:
                latents_ls.append(latents)

            # masking
            if apply_mask:
                latents = apply_mask(latents, t)

        latents_ls.append(latents)
        return latents_ls

    @torch.no_grad()
    def __call__(
        self,
        pipeline_type: Union[Txt2Img, Img2Img, Inpaint],
        text_input: BaseEncoding,
        guidance_scale: float = 7.5,
        num_inference_steps: int = 50,
        eta: float = 0.0,
        **kwargs,
    ):
        """
        Function invoked when calling the pipeline for generation.
        Args:
            pipeline_type: (`Txt2Img` or `Img2Img` or `Inpaint`):
                The pipeline execution type.
            text_input: (`BaseEncoding` and its subclasses):
                Textual input and how it is encoded
            guidance_scale (`float`, *optional*, defaults to 7.5):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`, usually at the expense of lower image quality.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (Î·) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to [`schedulers.DDIMScheduler`], will be ignored for others.
        Returns:
            generated images
        """
        with autocast(self._devicetype_str):
            # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
            # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
            # corresponds to doing no classifier free guidance.
            do_classifier_free_guidance = guidance_scale > 1.0

            text_embeddings = self.text_model.GetTextEmbeddings(
                text_input, do_classifier_free_guidance
            )

            latents_ls = self.Generate(
                text_embeddings,
                do_classifier_free_guidance,
                pipeline_type,
                guidance_scale,
                num_inference_steps,
                eta,
            )

            return [self.image_model.Decode(latents)[0] for latents in latents_ls]
