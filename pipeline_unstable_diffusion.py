# @title UnstableDiffusionPipeline
# See the following web page for the usage.
# https://github.com/nanashi161382/unstable_diffusion/tree/main
from diffusers import (
    StableDiffusionInpaintPipelineLegacy,
    DDIMScheduler,
    DiffusionPipeline,
    DPMSolverMultistepScheduler,
)
import inspect
import IPython
from IPython.display import display
import numpy as np
import PIL
import torch
from torch import autocast
from typing import Optional, List, Union, Callable, Tuple


class PipelineType:
    def __init__(self, rand_seed: Optional[int]):
        """
        Args:
            rand_seed (`int`, *optional*):
                A random seed for [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation deterministic.
        """
        self._rand_seed = rand_seed
        self._generator = None

    def InitializeGenerator(self, pipe):
        if not self._rand_seed:
            return None
        if not self._generator:
            self._generator = torch.Generator(device=pipe.device.type)
        self._generator.manual_seed(self._rand_seed)
        print(f"Setting random seed to {self._rand_seed}")

    def GetGenerator(self):
        return self._generator

    def Rand(self, shape, device, dtype):
        generator = self.GetGenerator()
        if device.type == "mps":
            # randn does not work reproducibly on mps
            return torch.randn(
                shape,
                generator=generator,
                device="cpu",
                dtype=dtype,
            ).to(device)
        else:
            return torch.randn(
                shape,
                generator=generator,
                device=device,
                dtype=dtype,
            )

    # Modified a copy from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline.get_timesteps
    def GetTimestepsWithStrength(self, pipe, num_inference_steps, strength):
        # get the original timestep using init_timestep
        offset = pipe.scheduler.config.get("steps_offset", 0)
        init_timestep = int(num_inference_steps * strength) + offset
        init_timestep = min(init_timestep, num_inference_steps)

        # Some schedulers like PNDM have timesteps as arrays
        # It's more optimized to move all timesteps to correct device beforehand
        t_start = max(num_inference_steps - init_timestep + offset, 0)
        timesteps = pipe.scheduler.timesteps[t_start:].to(pipe.device)

        latent_timestep = timesteps[:1]

        return timesteps, latent_timestep

    @classmethod
    def GetLatentsShape(
        cls, size: Tuple[int, int], batch_size, num_channels, scale_factor
    ):
        """
        Args:
            size (`(int, int)`, *optional*, defaults to (512, 512))
                The (width, height) pair in pixels of the generated image.
            batch_size:
                Always 1
            num_channels:
                unet.in_channels
            scale_factor:
                VAE scale factor
        """
        width, height = size
        if height % scale_factor != 0 or width % scale_factor != 0:
            print(
                f"`width` and `height` have to be divisible by {scale_factor}. "
                "Automatically rounded."
            )

        # get the initial random noise unless the user supplied it
        latents_shape = (
            batch_size,
            num_channels,
            height // scale_factor,
            width // scale_factor,
        )
        return latents_shape


class ByLatents:
    def __init__(self, latents: torch.FloatTensor):
        """
        Args:
            latents (`torch.FloatTensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image generation. Can be used to tweak the same generation with different prompts. If not provided, a latents tensor will ge generated by sampling using the supplied random `generator`.
        """
        self._latents = latents

    def GetLatents(self, txt2img, latents_shape, device, dtype):
        if latents.shape != latents_shape:
            raise ValueError(
                f"Unexpected latents shape, got {latents.shape}, expected {latents_shape}"
            )
        return latents.to(device=device, dtype=dtype)


class Randomly:
    def __init__(self, symmetric: Optional[bool] = False):
        self._symmetric = symmetric

    def GetLatents(self, txt2img, latents_shape, device, dtype):
        if not self._symmetric:
            latents = txt2img.Rand(latents_shape, device, dtype)
            return latents

        # Making symmetric latents.
        # `(latents + latents.flip(-1)) / 2.0` didn't work.
        width = latents_shape[-1]
        half_width = int((width + 1) / 2)
        half_shape = latents_shape[:-1] + (half_width,)
        left = txt2img.Rand(half_shape, device, dtype)
        right = left.flip(-1)
        extra_width = (half_width * 2) - width
        if extra_width > 0:
            right = right[:, :, :, extra_width:]
        return torch.cat([left, right], dim=-1)


class Txt2Img(PipelineType):
    def __init__(
        self,
        initialize: Union[ByLatents, Randomly],
        size: Tuple[int, int] = (512, 512),
        rand_seed: Optional[int] = None,
    ):
        """
        Args:
            size (`(int, int)`, *optional*, defaults to (512, 512))
                The (width, height) pair in pixels of the generated image.
            initialize (`ByLatents` or `Randomly`):
            rand_seed (`int`, *optional*):
                A random seed for [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation deterministic.
        """
        super().__init__(rand_seed)
        self._size = size
        self._initialize = initialize

    def GetInitialLatentsAndTimesteps(self, pipe, dtype, num_inference_steps: int):
        scale_factor = pipe.image_model.vae_scale_factor()

        latents_shape = PipelineType.GetLatentsShape(
            self._size, 1, pipe.unet.in_channels, scale_factor
        )
        latents = self._initialize.GetLatents(self, latents_shape, pipe.device, dtype)

        # scale the initial noise by the standard deviation required by the scheduler
        latents = latents * pipe.scheduler.init_noise_sigma

        # Some schedulers like PNDM have timesteps as arrays
        # It's more optimized to move all timesteps to correct device beforehand
        timesteps = pipe.scheduler.timesteps.to(pipe.device)

        return latents, timesteps, None


class Img2Img(PipelineType):
    def __init__(
        self,
        init_image: PIL.Image.Image,
        strength: float = 0.8,
        rand_seed: Optional[int] = None,
    ):
        """
        Args:
            init_image `PIL.Image.Image`:
                `Image`, or tensor representing an image batch, that will be used as the starting point for the process.
            strength (`float`, *optional*, defaults to 0.8):
                Conceptually, indicates how much to transform the reference `init_image`. Must be between 0 and 1.
                `init_image` will be used as a starting point, adding more noise to it the larger the `strength`. The number of denoising steps depends on the amount of noise initially added. When `strength` is 1, added noise will be maximum and the denoising process will run for the full number of iterations specified in `num_inference_steps`. A value of 1, therefore, essentially ignores `init_image`.
            rand_seed (`int`, *optional*):
                A random seed for [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation deterministic.
        """
        super().__init__(rand_seed)
        if strength < 0 or strength > 1:
            raise ValueError(
                f"The value of strength should in [0.0, 1.0] but is {strength}"
            )
        self._strength = strength
        self._init_image = init_image

    def GetInitialLatentsAndTimesteps(self, pipe, dtype, num_inference_steps: int):
        generator = self.GetGenerator()

        init_latents = pipe.image_model.Encode(self._init_image, dtype, generator)
        noise = self.Rand(init_latents.shape, pipe.device, dtype)
        timesteps, latent_timestep = self.GetTimestepsWithStrength(
            pipe, num_inference_steps, self._strength
        )

        init_latents_noise = pipe.scheduler.add_noise(
            init_latents, noise, latent_timestep
        )

        return init_latents_noise, timesteps, None


class Inpaint(PipelineType):
    def __init__(
        self,
        init_image: PIL.Image.Image,
        mask_image: PIL.Image.Image,
        strength: float = 0.8,
        rand_seed: Optional[int] = None,
    ):
        """
        Args:
            init_image `PIL.Image.Image`:
                `Image`, or tensor representing an image batch, that will be used as the starting point for the process. This is the image whose masked region will be inpainted.
            mask_image `PIL.Image.Image`:
                `Image`, or tensor representing an image batch, to mask `init_image`. White pixels in the mask will be replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.
            strength (`float`, *optional*, defaults to 0.8):
                Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When `strength` is 1, the denoising process will be run on the masked area for the full number of iterations specified in `num_inference_steps`. `init_image` will be used as a reference for the masked area, adding more noise to that region the larger the `strength`. If `strength` is 0, no inpainting will occur.
            rand_seed (`int`, *optional*):
                A random seed for [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation deterministic.
        """
        super().__init__(rand_seed)
        if strength < 0 or strength > 1:
            raise ValueError(
                f"The value of strength should in [0.0, 1.0] but is {strength}"
            )
        self._strength = strength
        self._init_image = init_image
        self._mask_image = mask_image

    def GetInitialLatentsAndTimesteps(self, pipe, dtype, num_inference_steps: int):
        generator = self.GetGenerator()

        init_latents = pipe.image_model.Encode(self._init_image, dtype, generator)
        mask = pipe.image_model.PreprocessMask(self._mask_image, dtype)
        noise = self.Rand(init_latents.shape, pipe.device, dtype)
        apply_mask = self.ApplyMask(pipe, init_latents, mask, noise)

        # check sizes
        if not mask.shape == init_latents.shape:
            raise ValueError("The mask and init_image should be the same size!")

        timesteps, latent_timestep = self.GetTimestepsWithStrength(
            pipe, num_inference_steps, self._strength
        )

        # add noise to latents using the timesteps
        init_latents_noise = apply_mask.AddNoise(latent_timestep)

        return init_latents_noise, timesteps, apply_mask

    class ApplyMask:
        def __init__(self, pipe, init_latents, mask, noise):
            self._pipe = pipe
            self._init_latents = init_latents
            self._mask = mask
            self._noise = noise

        def AddNoise(self, ts):
            return self._pipe.scheduler.add_noise(self._init_latents, self._noise, ts)

        def __call__(self, latents, t):
            init_latents_noise = self.AddNoise(torch.tensor([t]))
            return (init_latents_noise * self._mask) + (latents * (1 - self._mask))


class ImageModel:
    def __init__(self, vae, vae_scale_factor, device):
        """
        Args:
            vae:
            vae_scale_factor:
            device:
        """
        self._vae = vae
        self._vae_scale_factor = vae_scale_factor
        self._device = device

    def vae_scale_factor(self):
        return self._vae_scale_factor

    def Preprocess(self, image: PIL.Image.Image):
        w, h = image.size
        # Shouldn't this be consistent with vae_scale_factor?
        w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32
        image = image.resize((w, h), resample=PIL.Image.LANCZOS)
        image = np.array(image).astype(np.float32) / 255.0
        image = image[None].transpose(0, 3, 1, 2)
        image = torch.from_numpy(image)
        return 2.0 * image - 1.0

    def Encode(
        self, image: PIL.Image.Image, dtype, generator: Optional[torch.Generator]
    ):
        image = self.Preprocess(image).to(device=self._device, dtype=dtype)

        # encode the init image into latents and scale the latents
        latent_dist = self._vae.encode(image).latent_dist
        latents = latent_dist.sample(generator=generator)
        latents = 0.18215 * latents

        return latents

    def Decode(self, latents):
        latents = 1 / 0.18215 * latents
        image = self._vae.decode(latents).sample
        image = (image / 2 + 0.5).clamp(0, 1)
        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16
        image = image.cpu().permute(0, 2, 3, 1).float().numpy()
        return DiffusionPipeline.numpy_to_pil(image)

    def PreprocessMask(self, mask: PIL.Image.Image, dtype):
        scale_factor = self.vae_scale_factor()
        # preprocess mask
        mask = mask.convert("L")
        w, h = mask.size
        # Shouldn't this be consistent with vae_scale_factor?
        w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32
        mask = mask.resize(
            (w // scale_factor, h // scale_factor), resample=PIL.Image.NEAREST
        )
        mask = np.array(mask).astype(np.float32) / 255.0
        mask = np.tile(mask, (4, 1, 1))
        mask = mask[None].transpose(0, 1, 2, 3)  # what does this step do?
        mask = 1 - mask  # repaint white, keep black
        mask = torch.from_numpy(mask).to(device=self._device, dtype=dtype)
        return mask


class StandardEncoding:
    def __init__(
        self,
        prompt: str,
        negative_prompt: Optional[str] = None,
    ):
        """
        Args:
            prompt (`str`):
                The prompt or prompts to guide the image generation.
            negative_prompt (`str`, *optional*):
                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).
        """
        self._prompt = prompt
        if negative_prompt is None:
            negative_prompt = ""
        self._negative_prompt = negative_prompt

    def EncodeText(self, text_model, text: str):
        return text_model.EncodeText(text)[0]

    def EncodePrompt(self, text_model):
        return self.EncodeText(text_model, self._prompt)

    def EncodeNegativePrompt(self, text_model):
        return self.EncodeText(text_model, self._negative_prompt)

    # *** Utility functions for subclasses below ***
    @classmethod
    def AddOrInit(cls, v, x):
        if v is None:
            return x
        else:
            return v + x

    @classmethod
    def GetAnnotation(cls, chunk: str):
        words = []
        weight = 1.0
        repeat = 1
        for word in [x.strip() for x in chunk.split(" ")]:
            if not word:
                continue
            if word[0] == ":":
                if len(word) == 1:
                    raise ValueError(f"Don't put any spaces after :")
                weight = float(word[1:])
            elif word[0] == "*":
                if len(word) == 1:
                    raise ValueError(f"Don't put any spaces after *")
                repeat = int(word[1:])
            else:
                words.append(word)
        if weight < 0.0:
            raise ValueError(f"Invalid weight: {weight}")
        if repeat < 1:
            raise ValueError(f"Invalid repeat: {repeat}")
        return " ".join(words), weight, repeat

    class EncodedSegment:
        def __init__(self, embeddings, weight):
            self.weight = torch.tensor(weight).to(embeddings[0].dtype)
            self.unweighted_mean = (
                embeddings[0][0].float().mean(axis=[-2, -1]).to(embeddings[0].dtype)
            )
            self.full = embeddings[0][0] * weight
            self.sot = self.full[0]
            self.eot = embeddings[1][0] * weight
            self.end = self.full[-1]
            effective_length = embeddings[2]
            self.words = self.full[1:effective_length]

        @classmethod
        def Encode(cls, text_model, text, weight, fail_on_truncation: bool = True):
            embeddings = text_model.EncodeText(text, fail_on_truncation)
            return cls(embeddings, weight)

        @classmethod
        def AnnotateAndEncode(cls, text_model, chunk, fail_on_truncation: bool = True):
            phrase, weight, repeat = StandardEncoding.GetAnnotation(chunk)
            display([phrase, weight, repeat])
            return cls.Encode(text_model, phrase, weight, fail_on_truncation), repeat

    class AdjustingBase:
        def __init__(self):
            self.unweighted_mean = None

        def Add(self, enc, unweighted_mean):
            self.unweighted_mean = enc.AddOrInit(self.unweighted_mean, unweighted_mean)

        def Average(self, chunk_len):
            chunk_len = torch.tensor(chunk_len).to(self.unweighted_mean.dtype)
            self.unweighted_mean /= chunk_len

        def AdjustMean(self, states, device):
            new_mean = states.float().mean(axis=[-2, -1]).to(states.dtype)
            states *= (self.unweighted_mean / new_mean).unsqueeze(-1)
            display(states.shape)
            display(states)
            return torch.cat([torch.unsqueeze(states, 0)], dim=0).to(device)

    class SequentialBase(AdjustingBase):
        def __init__(self):
            super().__init__()
            self.sot_state = None
            self.eot_state = None
            self.end_state = None
            self.word_states = []
            self.word_states_len = 0
            self.eot_states = []

        def Add(self, enc, segment):
            super().Add(enc, segment.unweighted_mean)
            self.sot_state = enc.AddOrInit(self.sot_state, segment.sot)
            self.eot_state = enc.AddOrInit(self.eot_state, segment.eot)
            self.end_state = enc.AddOrInit(self.end_state, segment.end)
            self.word_states.append(segment.words)
            self.word_states_len += segment.words.shape[0]
            self.eot_states.append(segment.eot.unsqueeze(0))

        def Average(self, chunk_len):
            super().Average(chunk_len)
            chunk_len = torch.tensor(chunk_len).to(self.unweighted_mean.dtype)
            self.sot_state /= chunk_len
            self.eot_state /= chunk_len
            self.end_state /= chunk_len

    class Parallel(AdjustingBase):
        def __init__(self):
            super().__init__()
            self.states = None

        def Add(self, enc, segment):
            super().Add(enc, segment.unweighted_mean)
            self.states = enc.AddOrInit(self.states, segment.full)

        def Average(self, chunk_len):
            super().Average(chunk_len)
            self.states /= chunk_len

        def ToStates(self, *input, **kwargs):
            return self.states


class EotEncoding(StandardEncoding):
    def __init__(
        self,
        prompt: str,
        negative_prompt: Optional[str] = None,
        repeat: bool = False,
    ):
        """
        Args:
            prompt (`str`):
                The prompt or prompts to guide the image generation.
            negative_prompt (`str`, *optional*):
                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).
        """
        super().__init__(prompt, negative_prompt)
        self._repeat = repeat

    def EncodeText(self, text_model, text: str):
        embeddings = text_model.EncodeText(text)
        full_states = embeddings[0][0]
        eot_state = embeddings[1][0]
        end_state = full_states[-1]

        old_mean = full_states.float().mean(axis=[-2, -1]).to(full_states.dtype)
        if self._repeat:
            new_states = torch.cat(
                (
                    full_states[0].unsqueeze(0),
                    eot_state.repeat((len(full_states) - 2), 1),
                    end_state.unsqueeze(0),
                ),
                dim=0,
            )
        else:
            new_states = torch.cat(
                (
                    full_states[0].unsqueeze(0),
                    eot_state.unsqueeze(0),
                    end_state.repeat((len(full_states) - 2), 1),
                ),
                dim=0,
            )
        new_mean = new_states.float().mean(axis=[-2, -1]).to(new_states.dtype)
        new_states *= (old_mean / new_mean).unsqueeze(-1)
        display(new_states.shape)
        display(new_states)
        return torch.cat([torch.unsqueeze(new_states, 0)], dim=0).to(text_model._device)


class SegmentedEotEncoding(StandardEncoding):
    def __init__(
        self,
        prompt: str,
        negative_prompt: Optional[str] = None,
        repeat: bool = False,
    ):
        """
        Args:
            prompt (`str`):
                The prompt or prompts to guide the image generation.
            negative_prompt (`str`, *optional*):
                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).
        """
        super().__init__(prompt, negative_prompt)
        self._repeat = repeat

    class Sequential(StandardEncoding.SequentialBase):
        def ToStates(self, max_length, repeat):
            states = (
                [self.sot_state.unsqueeze(0)]
                + (self.eot_states * repeat)
                + [self.eot_state.unsqueeze(0)]
            )
            states = torch.cat(
                states + ([self.end_state.unsqueeze(0)] * (max_length - len(states))),
                dim=0,
            )
            return states

    def EncodeText(self, text_model, text: str):
        chunks = [x.strip() for x in text.split(",")]
        display(chunks)

        proc = self.Sequential()
        chunk_len = 0
        for chunk in chunks:
            segment, repeat = self.EncodedSegment.AnnotateAndEncode(text_model, chunk)
            chunk_len += repeat
            for i in range(repeat):
                proc.Add(self, segment)
        proc.Average(chunk_len)

        eot_repeat = 1
        if self._repeat:
            eot_repeat = int((text_model.max_length() - 3) / len(proc.eot_states))
        new_states = proc.ToStates(text_model.max_length(), eot_repeat)
        return proc.AdjustMean(new_states, text_model._device)


class SegmentedEncoding(StandardEncoding):
    def __init__(
        self,
        prompt: str,
        negative_prompt: Optional[str] = None,
        sequential: bool = True,
    ):
        """
        Args:
            prompt (`str`):
                The prompt or prompts to guide the image generation.
            negative_prompt (`str`, *optional*):
                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).
            sequential (`bool`):
        """
        super().__init__(prompt, negative_prompt)
        self._sequential = sequential

    class Sequential(StandardEncoding.SequentialBase):
        def ToStates(self, max_length):
            states = (
                [self.sot_state.unsqueeze(0)]
                + self.word_states
                + [self.eot_state.unsqueeze(0)]
            )
            states_len = self.word_states_len + 2
            states = torch.cat(
                states + ([self.end_state.unsqueeze(0)] * (max_length - states_len)),
                dim=0,
            )
            return states

    def EncodeText(self, text_model, text: str):
        chunks = [x.strip() for x in text.split(",")]
        display(chunks)

        if self._sequential:
            proc = self.Sequential()
        else:
            proc = self.Parallel()

        chunk_len = 0
        for chunk in chunks:
            segment, repeat = self.EncodedSegment.AnnotateAndEncode(text_model, chunk)
            chunk_len += repeat
            for i in range(repeat):
                proc.Add(self, segment)
        proc.Average(chunk_len)

        new_states = proc.ToStates(text_model.max_length())
        return proc.AdjustMean(new_states, text_model._device)


class RotatedEncoding(StandardEncoding):
    class Parallel(StandardEncoding.AdjustingBase):
        def __init__(self):
            super().__init__()
            self.states = None

        def Add(self, enc, states, unweighted_mean):
            super().Add(enc, unweighted_mean)
            self.states = enc.AddOrInit(self.states, states)

        def Average(self, chunk_len):
            super().Average(chunk_len)
            self.states /= chunk_len

        def ToStates(self, *input, **kwargs):
            return self.states

    def EncodeText(self, text_model, text: str):
        chunks = [x.strip() for x in text.split(",")]
        display(chunks)

        annotations = [StandardEncoding.GetAnnotation(chunk) for chunk in chunks]
        all_phrases = []
        all_weights = []
        all_token_nums = []
        for phrase, weight, repeat in annotations:
            num_tokens = text_model.num_tokens(phrase)
            for i in range(repeat):
                all_phrases.append(phrase)
                all_weights.append(weight)
                all_token_nums.append(num_tokens)
        display(all_phrases)
        display(all_weights)
        display(all_token_nums)

        proc = self.Parallel()

        def run():
            for i in range(len(all_phrases)):
                rotated_text = " ".join(all_phrases[i:] + all_phrases[:i])
                weights = [1.0]
                for w, n in zip(
                    all_weights[i:] + all_weights[:i],
                    all_token_nums[i:] + all_token_nums[:i],
                ):
                    weights += [w] * n
                display([weights, rotated_text])
                weights += [1.0] * (text_model.max_length() - len(weights))
                weights = torch.tensor(weights).to(text_model._device)
                embeddings = text_model.EncodeText(rotated_text, False)
                unweighted_mean = (
                    embeddings[0][0].float().mean(axis=[-2, -1]).to(embeddings[0].dtype)
                )
                full = embeddings[0][0] * weights.unsqueeze(-1)
                proc.Add(self, full, unweighted_mean)

        run()
        all_phrases.reverse()
        all_weights.reverse()
        all_token_nums.reverse()
        run()
        proc.Average(len(all_phrases) * 2)

        new_states = proc.ToStates(text_model.max_length(), 1)
        return proc.AdjustMean(new_states, text_model._device)


class TextModel:
    def __init__(self, tokenizer, text_encoder, device):
        """
        Args:
            tokenizer:
            text_encoder:
            device:
        """
        self._tokenizer = tokenizer
        self._text_encoder = text_encoder
        self._device = device

    def max_length(self):
        return self._tokenizer.model_max_length

    def num_tokens(self, text: str):
        tokens = self._tokenizer(text)
        display(tokens.input_ids)
        return len(tokens.input_ids) - 2

    def tokenize(self, text: str):
        max_length = self.max_length()
        return self._tokenizer(
            text,
            padding="max_length",
            max_length=max_length,
            return_tensors="pt",
        )

    def decode_tokens(self, tokens):
        return self._tokenizer.batch_decode(tokens)

    def EncodeText(self, text: str, fail_on_truncation: bool = True):
        max_length = self.max_length()
        text_inputs = self.tokenize(text)
        text_input_ids = text_inputs.input_ids
        attention_mask = text_inputs.attention_mask

        if text_input_ids.shape[-1] > max_length:
            removed_text = self.decode_tokens(text_input_ids[:, max_length:])
            if fail_on_truncation:
                raise ValueError(
                    "The following part of your input was truncated because CLIP can only handle sequences up to"
                    f" {max_length} tokens: {removed_text}"
                )
            text_input_ids = text_input_ids[:, :max_length]
        embeddings = self._text_encoder(
            text_input_ids.to(self._device),
            attention_mask=None,
        )

        num_tokens = 0
        for m in attention_mask[0]:
            if m.item() > 0:
                num_tokens += 1
        return embeddings[0], embeddings[1], num_tokens

    def GetTextEmbeddings(
        self,
        text_input: StandardEncoding,
        do_classifier_free_guidance: bool,
    ):
        # get prompt text embeddings
        text_embeddings = text_input.EncodePrompt(self)

        # get unconditional embeddings for classifier free guidance
        if do_classifier_free_guidance:
            uncond_embeddings = text_input.EncodeNegativePrompt(self)

            # For classifier free guidance, we need to do two forward passes.
            # Here we concatenate the unconditional and text embeddings into a single batch
            # to avoid doing two forward passes
            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

        return text_embeddings


class UnstableDiffusionPipeline:
    def __init__(self, devicetype: str = "cuda"):
        self._devicetype_str = devicetype

    def GetRevision(self, dataset: str, revision: Optional[str] = None):
        # revision is a git branch name assigned to the model repository.

        # Always return the provided revision if any.
        if revision:
            return revision

        # We always have the "main" branch.
        default_revision = "main"
        # There may be other branches for smaller memory footprint
        recommended_revision = {
            "stabilityai/stable-diffusion-2": "fp16",
            "CompVis/stable-diffusion-v1-4": "fp16",
            "runwayml/stable-diffusion-v1-5": "fp16",
            "hakurei/waifu-diffusion": "fp16",
            "naclbit/trinart_stable_diffusion_v2": "diffusers-60k",
        }

        return recommended_revision.get(dataset, default_revision)

    def Connect(
        self,
        dataset: str,
        revision: Optional[str] = None,
        auth_token: Optional[str] = None,
        use_xformers: bool = False,
    ):
        extra_args = {
            "torch_dtype": torch.float32,
            "revision": self.GetRevision(dataset, revision),
        }
        if auth_token:
            extra_args["use_auth_token"] = auth_token

        # use DPM-Solver++ scheduler
        scheduler = DPMSolverMultistepScheduler.from_pretrained(
            dataset, subfolder="scheduler"
        )
        extra_args["scheduler"] = scheduler

        # Prepare the StableDiffusion pipeline.
        pipe = StableDiffusionInpaintPipelineLegacy.from_pretrained(
            dataset, **extra_args
        ).to(self._devicetype_str)

        # Options for efficient execution
        pipe.enable_attention_slicing()
        if use_xformers and (self._devicetype_str == "cuda"):
            pipe.enable_xformers_memory_efficient_attention()

        return self.SetPipeline(pipe)

    def SetPipeline(self, pipe):
        self._pipe = pipe
        self.device = self._pipe.device
        self.unet = self._pipe.unet
        self.scheduler = self._pipe.scheduler
        self.text_model = TextModel(pipe.tokenizer, pipe.text_encoder, pipe.device)
        self.image_model = ImageModel(pipe.vae, pipe.vae_scale_factor, pipe.device)
        return self

    def progress_bar(self, *input, **kwargs):
        return self._pipe.progress_bar(*input, **kwargs)

    @torch.no_grad()
    def __call__(
        self,
        pipeline_type: Union[Txt2Img, Img2Img, Inpaint],
        text_input: StandardEncoding,
        guidance_scale: float = 7.5,
        num_inference_steps: int = 50,
        eta: float = 0.0,
        **kwargs,
    ):
        """
        Function invoked when calling the pipeline for generation.
        Args:
            pipeline_type: (`Txt2Img` or `Img2Img` or `Inpaint`):
                The pipeline execution type.
            text_input: (`StandardEncoding` and its subclasses):
                Textual input and how it is encoded
            guidance_scale (`float`, *optional*, defaults to 7.5):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`, usually at the expense of lower image quality.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to [`schedulers.DDIMScheduler`], will be ignored for others.
        Returns:
            generated images
        """
        with autocast(self._devicetype_str):
            pipeline_type.InitializeGenerator(self)

            # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
            # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
            # corresponds to doing no classifier free guidance.
            do_classifier_free_guidance = guidance_scale > 1.0

            text_embeddings = self.text_model.GetTextEmbeddings(
                text_input, do_classifier_free_guidance
            )

            # set timesteps and initial latents
            self.scheduler.set_timesteps(num_inference_steps)
            (
                latents,
                timesteps,
                apply_mask,
            ) = pipeline_type.GetInitialLatentsAndTimesteps(
                self, text_embeddings.dtype, num_inference_steps
            )

            # 8. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
            extra_step_kwargs = self._pipe.prepare_extra_step_kwargs(
                pipeline_type.GetGenerator(), eta
            )

            for i, t in enumerate(self.progress_bar(timesteps)):
                # expand the latents if we are doing classifier free guidance
                latent_model_input = (
                    torch.cat([latents] * 2) if do_classifier_free_guidance else latents
                )
                latent_model_input = self.scheduler.scale_model_input(
                    latent_model_input, t
                )

                # predict the noise residual
                noise_pred = self.unet(
                    latent_model_input, t, encoder_hidden_states=text_embeddings
                ).sample

                # perform guidance
                if do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )

                # compute the previous noisy sample x_t -> x_t-1
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs
                ).prev_sample

                # masking
                if apply_mask:
                    latents = apply_mask(latents, t)

            return self.image_model.Decode(latents)
